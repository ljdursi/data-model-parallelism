{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0be144bc",
   "metadata": {},
   "source": [
    "# Next Steps and Other Resources\n",
    "\n",
    "We've now seen an overview of some fundamental distributed training methods in PyTorch, and when to use them:\n",
    "\n",
    "| Framework | Model must fit on single GPU | Parallelism  | Code changess required | Communications cost |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Distributed Data Parallel   | True   | Data - over input batches | Low  |  Modest |\n",
    "| Tensor Parallelism          | False  | Model - over layers       | High |  Low    |\n",
    "| Fully Sharded Data Parallel | False  | Data - over input batches | Modest, but many options | High |\n",
    "\n",
    "Relatedly, we've learned quite a bit about `torch.distributed` and torchrun.\n",
    "\n",
    "There's lots of next steps you can follow, depending on your interests.\n",
    "\n",
    "* The next steps at the end of each lab are worth pursuing, suggesting ways to deepen your understanding of that particular tool\n",
    "* \"[The Ultra-Scale Playbook: Training LLMs on GPU Clusters](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high-level_overview)\" by Tazi _et al_ on Hugging face is a monumental overview of techniques for distributed training. While the target for the playbook is LLMs, many of the approaches translate quite broadly.   The material is quite dense --- readers will probably have to look up terms and use links within as a launching point for further explanation --- but it is a fantastic overiew of the current state of the art.\n",
    "* The [Pytorch Distributed and Parallel Training tutorials](https://docs.pytorch.org/tutorials/distributed/home.html) inspired much of the presentation of this material, and some of the examples there show up in tweaked form here."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
