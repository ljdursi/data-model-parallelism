{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e2a7ef7",
   "metadata": {},
   "source": [
    "# Pipeline Parallelism\n",
    "\n",
    "In this section we'll go over the basic ideas of model parallelization by placing different layers of the model on different devices.  We'll also briefly cover an example using the under-development and subject-to-change [`torch.distributed.pipelining`](https://docs.pytorch.org/docs/stable/distributed.pipelining.html) API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462041c",
   "metadata": {},
   "source": [
    "## Model Parallelism by Layers\n",
    "\n",
    "The basic idea of layer-based parallelism is to tackle models larger than what a single GPU can fit by splitting the model into units of layers, and propagating the forward and backward passes through those units:\n",
    "\n",
    "![Diagram of mmodel parallelism by layer, where a model is broken into three units distributed over three GPUs. A forward pass sweeps through in one direction, followed by a backward pass in the other](images/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485ea305",
   "metadata": {},
   "source": [
    "This approach has much to commend it.  In particular, the communication-to-computation ratio can be quite favourable.  The only data that needs to be propagated are the activations (forward pass) or gradients (backward pass) at the boundary of the unit, while the volume of the unit is what needs to be computed on.   This makes it very very well suited for parallelism across nodes; the bandwidth and latency requirements for communication can be quite modest.\n",
    "\n",
    "![Diagram of layer-based parallelism with and without microbatches.  Without breaking batches up the GPUs are idle much of the time.  By breaking SGD-style minibatches further up into microbatches, the GPU utilization is much improved](images/pipeline-microbatches.png)\n",
    "\n",
    "However, using our normal schedule of forward/backward passes through the GPUs would leave the GPUs idle most of the time.  For instance, if we were using 3 GPUs this way, each GPU would only be working 1/3 of the time, spending the other 2/3 waiting for other units on other GPUs to complete.   This gets worse with more GPUs!\n",
    "\n",
    "A very common approach in parallel computing when latency becomes an issue - here while waiting for other units on other GPUs to complete - is [pipelining](https://en.wikipedia.org/wiki/Pipeline_(computing)), scheduling multiple computations to be in flight simultaneously, so that there is work to do during what would otherwise be idle cycles.\n",
    "\n",
    "In this context, we break our SGD-style minibatches into multiple microbatches, processing each of these sub-batches one at a time.  This fills up much of the \"bubbles\" of idle time, as shown in the figure above.  Gradient accumulation is done throughout each of the microbatches, and an update is performed when all the chunks are completed so that it behaves more like the model saw a single minibatch.\n",
    "\n",
    "As always when choosing batch sizes, there's a tradeoff.  More and smaller microbatches (increasing the number of chunks per minibatch) provides finer-grained parallelism and so reduces idle time, but too small may not take full advantage of the GPU.  Finding the right size is generally a matter of experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3217c0c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db3147fd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
