{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c31cbf45",
   "metadata": {},
   "source": [
    "## Data and Model Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2de0aa",
   "metadata": {},
   "source": [
    "This set of notebooks gives an overview of how to use several data and model parallelism approaches with PyTorch.  We cover:\n",
    "\n",
    "* Distributed data parallelism (DDP), using     the `torch.nn.parallel.DistributedDataParallel` module.\n",
    "* Pipeline parallelism, using the `torch.distributed.pipeline` module.\n",
    "* Fully Sharded Data Parallel (FSDP), using the `torch.distributed.fsdp` module.\n",
    "\n",
    "and to support that, we also discuss `torchrun` in smod detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534fc368",
   "metadata": {},
   "source": [
    "The notebooks are designed to be run in an environment where multiple GPUs are available.  The distributed examples make by default make use of 3 or 4 GPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd9b754",
   "metadata": {},
   "source": [
    "The notebooks are:\n",
    "* [0_Start_Here](0_Start_Here.ipynb): You are here, the table of contents\n",
    "* [1_EuroSAT_single_gpu](1_EuroSAT_single_gpu.ipynb): A single GPU interactive example of training a model on the EuroSAT dataset.\n",
    "* [2_Torchrun_and_distributed](2_Torchrun_and_distributed.ipynb): An introduction to the `torchrun` command we'll be using to run distributed training jobs; it works for single-node jobs (which is what we'll be doing) but also for multi-node jobs.\n",
    "* [3_Data_and_model_parallelism](3_Data_and_model_parallelism.ipynb): An introduction to data and model parallelism.  We'll talk about the difference between data and model parallelism, and give very brief introductinos to the idea behind each.\n",
    "* [4_Distributed_data_parallel](4_Distributed_data_parallel.ipynb): An introduction to distributed data parallelism, using the `torch.nn.parallel.DistributedDataParallel` module.   We'll walk through the steps of taking our single-GPU EuroSAT example and converting it to use DDP.  \n",
    "* [5_Pipelining](5_Pipelining.ipynb): An introduction to pipeline parallelism, using the `torch.distributed.pipeline` module.  We'll walk through the steps of taking our single-GPU EuroSAT example and converting it to use pipelining.\n",
    "* [6_Fully_sharded_data_parallel](6_Fully_sharded_data_parallel.ipynb): An introduction to fully sharded data parallelism, using the `torch.distributed.fsdp` module.  We'll walk through the steps of taking our single-GPU EuroSAT example and converting it to use FSDP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810ecf36",
   "metadata": {},
   "source": [
    "Let's go to the next notebook, [1_EuroSAT_single_gpu](1_EuroSAT_single_gpu.ipynb), to get started with a single GPU example of training a model on the EuroSAT dataset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
