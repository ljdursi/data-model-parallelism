{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Sharded Data Parallelism (FSDP2)\n",
    "\n",
    "## FSDP is Data Parallelism...\n",
    "\n",
    "Discussions about FSDP can get confusing, because it implements a lot of different techinques (some of them quite advanced).  But the first thing to know is that it is data parallelism, just as we've seen in [the DDP notebook](4_Distributed_data_parallel.ipynb), and as we saw in images there:\n",
    "\n",
    "![Overview of data parallelism](images/data-par-1.png)\n",
    "\n",
    "So each instance is responsible for training the entire model on a separate batch of data; you need something like DistributedSampler in your data loader, etc.   It's data parallelism.\n",
    "\n",
    "## ... and FSDP also uses Model Parallelism Techniques (Amongst Others) To Reduce Memory Usage\n",
    "\n",
    "FSDP differs by implementing a number of techniques to reduce memory usage, so that **FSDP can work even if the entire model won't fit on a single GPU**.  \n",
    "\n",
    "The signature method, sharding, means that each replica only persistantly stores a shard of the entire model, and state is materialized in place only when needed:\n",
    "\n",
    "![Sharded data parallelism](images/sharded-data-par-2.png)\n",
    "\n",
    "So that each GPU can be training a replica of a model which is, in principle, significantly larger than the memory of the GPU.\n",
    "\n",
    "My clumsy diagrams above probably make it look like it's only the model parameters which are sharded, but in fact GPU memory is required for parameters, gradients, and the potentially quite large optimizer state; all of those can be sharded (or at least not persisted:)\n",
    "\n",
    "![Diagram showing a memory-use graph demonstrating sharding of parameters, gradients, and optimizer state, from the ZeRO paper](images/ZeRO.png)\n",
    "\n",
    "The figure above is from the paper \"[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)\" which described and implemented these approaches; PyTorch's implementation of these methods is FSDP.   The math to the side sketches out the memory requirements; if there are $x$ parameters, and we're using FP16 (2 bytes per parameter), the sizes of the different layers are:\n",
    "\n",
    "* ParametersÂ - $2x$\n",
    "* Gradients - $2x$\n",
    "* Optimizer State (for, say, Adam) - $12x$\n",
    "  * Parameter copy $4x$ (4 bytes for float32)\n",
    "  * Momentum $4x$\n",
    "  * Variance $4x$\n",
    "\n",
    "FSDP uses [DTensors](https://docs.pytorch.org/docs/stable/distributed.tensor.html) and [Device Meshes](https://docs.pytorch.org/docs/stable/distributed.html#torch.distributed.device_mesh.DeviceMesh), from the Tensor parallism framework, to handle the sharding.   This isn't tensor parallelism, though; computation isn't parallelized over pieces of tensors.   Each of the replicas trains the entire over its subset of batches; it's the _persistant storage_ of shards of tensors which is distributed.\n",
    "\n",
    "\n",
    "## The FSDP2 workflow\n",
    "\n",
    "\n",
    "\n",
    "![FSDP combines tensor and pipeline parallelism; from the FSDP paper](images/fsdp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
