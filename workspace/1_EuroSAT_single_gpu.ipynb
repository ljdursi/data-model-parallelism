{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f7b3d34",
   "metadata": {},
   "source": [
    "# EuroSAT training example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afae1c6f",
   "metadata": {},
   "source": [
    "![EuroSAT dataset example images](images/EuroSAT-fig-4.png)\n",
    "\n",
    "This is a single-GPU version of a workflow for training a simple convolutional network to classify EuroSAT data; information about the dataset and the paper documenting it (from which the image above comes) can be found [at this GitHub repository](https://github.com/phelber/eurosat).\n",
    "\n",
    "Below we'll walk through the single-GPU training loop, and we will use this example to explore PyTorch's key distributed training frameworks --- [Distributed Data Parallel](https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html), [Pipeline Parallel](https://docs.pytorch.org/docs/stable/distributed.pipelining.html), and [Fully Sharded Data Parallel (FSDP2)](https://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html).\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8063de8",
   "metadata": {},
   "source": [
    "First let's import the libaries we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ba5c19-0c47-425c-b4bf-761aaf7cb33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f1f6a9",
   "metadata": {},
   "source": [
    "The dataset is in the torchvision library, so we can import it directly from there.  For setting up the data loaders, we'll want to set up some simple transforms.  Let's also, for convenience, set up a list of the category names we'll be using for the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c3a6f9-9916-4d79-9dde-4cfb6a0f2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToImage(),\n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "dataset = torchvision.datasets.EuroSAT(\n",
    "    root=\"./data\", download=True, transform=transform\n",
    ")\n",
    "total_count = len(dataset)\n",
    "train_count = int(0.6 * total_count)\n",
    "valid_count = int(0.2 * total_count)\n",
    "test_count = total_count - train_count - valid_count\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, (train_count, valid_count, test_count)\n",
    ")\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True\n",
    ")\n",
    "\n",
    "\n",
    "classes = [\n",
    "    \"AnnualCrop\",\n",
    "    \"Forest\",\n",
    "    \"HerbaceousVegetation\",\n",
    "    \"Highway\",\n",
    "    \"Industrial\",\n",
    "    \"Pasture\",\n",
    "    \"PermanentCrop\",\n",
    "    \"Residential\",\n",
    "    \"River\",\n",
    "    \"SeaLake\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce0f197",
   "metadata": {},
   "source": [
    "Alright, great.  Lets take a look at some of the items of data, so we know what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06607a7b-4f26-4dc6-ace3-1237d7475431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img * 0.224 + 0.456  # unnormalize\n",
    "    npimg = np.clip(img.numpy(), 0, 1)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(\" \".join(f\"{classes[labels[j]]:5s}\" for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c1d7ef",
   "metadata": {},
   "source": [
    "Great!  Ok, so now we can start building our model.  We'll use a simple convolutional neural network, which we'll define in the next cells.  We'll use a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18fc3f4-e73a-4421-be88-1ce4930a0405",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d76e1-3727-43a1-b806-7c88edc98502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3, out_channels=32, kernel_size=3, padding=1, stride=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=32, out_channels=32, kernel_size=3, padding=1, stride=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 64x64 -> 32x32\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32, out_channels=64, kernel_size=3, padding=1, stride=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=64, out_channels=64, kernel_size=3, padding=1, stride=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 32x32 -> 16x16\n",
    "        )\n",
    "\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=128, out_channels=128, kernel_size=3, padding=1, stride=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 16x16 -> 8x8\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling and Fully Connected Layers\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(\n",
    "            (1, 1)\n",
    "        )  # Reduces each 128-channel map to 1x1\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),  # Input will be (batch_size, 128)\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Standard dropout for FC layers\n",
    "            nn.Linear(in_features=64, out_features=num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0cc3c",
   "metadata": {},
   "source": [
    "Let's instantiate the model on the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5839e43-673c-4d7e-bb33-766fec2ea581",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(len(classes)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c193d85",
   "metadata": {},
   "source": [
    "Ok, next up is to set up the loss function and the optimizer.  We'll use cross-entropy loss, which is standard for classification tasks, and we'll use a simple SGD optimizer with a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01233c29-c5f6-4ab4-9d33-c329ff3e4e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98956927",
   "metadata": {},
   "source": [
    "Here's the routine to evaluate how well we're doing on the validation set.  We'll use this to monitor our progress during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fab0b4-2e44-4b36-8270-835f398ecc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, loss_fn, device):\n",
    "    total_labels = 0\n",
    "    correct_labels = 0\n",
    "    loss_total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            # Transfering images and labels to GPU if available\n",
    "            labels = labels.to(device)\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Extracting predicted label, and computing validation loss and validation accuracy\n",
    "            predictions = torch.max(outputs, 1)[1]\n",
    "            total_labels += len(labels)\n",
    "            correct_labels += (predictions == labels).sum()\n",
    "            loss_total += loss\n",
    "\n",
    "    v_accuracy = correct_labels / total_labels\n",
    "    v_loss = loss_total / len(test_loader)\n",
    "\n",
    "    return v_accuracy, v_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2905d02",
   "metadata": {},
   "source": [
    "Alright, so now let's set up and run the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b78b42c-502c-4a4f-a9fd-e6bc04578b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 0\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    t0 = time.time()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 500 == 499:  # print every 500 minibatches\n",
    "            print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # timing\n",
    "    epoch_time = time.time() - t0\n",
    "    total_time += epoch_time\n",
    "\n",
    "    # output metrics at the end of each epoch\n",
    "    images_per_sec = torch.tensor(len(trainloader) * batch_size / epoch_time).to(\n",
    "        device\n",
    "    )\n",
    "    v_accuracy, v_loss = test(net, testloader, criterion, device)\n",
    "    print(\n",
    "        f\"Epoch = {epoch:2d}: Cumulative Time = {total_time:5.3f}, Epoch Time = {epoch_time:5.3f}, Images/sec = {images_per_sec:5.3f}, Validation Loss = {v_loss:5.3f}, Validation Accuracy = {v_accuracy:5.3f}\"\n",
    "    )\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff8ce6",
   "metadata": {},
   "source": [
    "Let's save the model after training, so we can use it later for inference or further training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d3416c-c0a9-4472-b66a-dd6721f47766",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./eurosat_net.pth\"\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce97109",
   "metadata": {},
   "source": [
    "Here's some simple data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dfedbb-a8b0-4599-aa31-ce8e1aa73d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(\"GroundTruth: \", \" \".join(f\"{classes[labels[j]]:5s}\" for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7230f130",
   "metadata": {},
   "source": [
    "Now let's see how the model performs on those samples after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c17f33-7eb8-4c22-b339-4ef00a5c0891",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.to(device)\n",
    "net.load_state_dict(torch.load(PATH, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c1efd-7af7-49bd-84e7-31bb4e80e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.to(device)\n",
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9fa627-50d3-4958-90a3-86fa03aed5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "print(predicted)\n",
    "\n",
    "print(\"Predicted: \", \" \".join(f\"{classes[predicted[j]]:5s}\" for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30668c2d",
   "metadata": {},
   "source": [
    "Not bad!  Now let's look at how the model performs on all the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9929585d-fd62-4801-b0e3-d35ae2b862ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on the 10000 test images: {100 * correct // total} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76b4cbc",
   "metadata": {},
   "source": [
    "And now by category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b61815b-a580-4121-84ed-426737ecf9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f\"Accuracy for class: {classname:5s} is {accuracy:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894f0ec-26a9-4669-a033-0179807f48db",
   "metadata": {},
   "source": [
    "Nicely done.  There's experiments we can do to improve the model, such as using a more complex architecture, data augmentation, or hyperparameter tuning.  But this is a good start!\n",
    "\n",
    "Next up is applying distributed training techniques to this model.  But first, let's have a little introduction to the command we'll use to launch distributed training, torchrun, [in the next notebook](2_Torchrun_and_distributed)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
