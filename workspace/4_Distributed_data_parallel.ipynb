{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c5c335a",
   "metadata": {},
   "source": [
    "# Distributed Data Parallelism with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a94bbf5",
   "metadata": {},
   "source": [
    "In this section, we're going to explore how to use PyTorch's distributed data parallelism capabilities in the [DDP](https://docs.pytorch.org/docs/stable/notes/ddp.html) library to train a model across multiple GPUs. This is particularly useful for speeding up training processes by and reducing the time it takes to train large models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27a7d3d",
   "metadata": {},
   "source": [
    "## Communication Pattern w/ DDP\n",
    "\n",
    "The basic idea of DDP is to replicate the model on each GPU and then synchronize gradients across all GPUs during the backward pass. Each process computes gradients independently, and then they are averaged across all processes. This allows for efficient training without the need for a central parameter server.\n",
    "\n",
    "![DDP Communication pattern - image from original FSDP paper by Facebook](images/ddp.png) \n",
    "\n",
    "### Wiring up the communications\n",
    "\n",
    "To enable this, the first thing we have to do is to set up the process group so that each process handling a replica can communicate with each other.   This is going to be done exactly as with our simple communication scripts we ran with `torchrun` in the [Torchrun section](2_Torchrun_and_distributed.ipynb)\n",
    "\n",
    "So as with (say) the [reductions.py example](code/reductions.py), we need to set up the process group with something like:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import torch.distributed as dist\n",
    "\n",
    "global_rank = int(os.environ.get(\"RANK\", 0))\n",
    "local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "\n",
    "device = torch.device(f\"cuda:{local_rank}\")\n",
    "dist.init_process_group(backend=\"nccl\", world_size=world_size, rank=global_rank, device_id=device)\n",
    "```\n",
    "\n",
    "and then of course call `dist.destroy_process_group()` at the end of the script to clean up.\n",
    "\n",
    "### Wrapping the model\n",
    "\n",
    "For actually doing the gradient synchronization and implementing that in the training loop, we're in luck!\n",
    "Implementing that manually would take some doing, but DistributedDataParallel offers a very simple high-level API to do this - you simply wrap your model with `torch.nn.parallel.DistributedDataParallel` and PyTorch handles the \n",
    "communication steps for you.\n",
    "\n",
    "So for instance, in the first, simple example we're going to look at, the single GPU code is essentially:\n",
    "\n",
    "```python\n",
    "    model = torch.nn.Linear(20, 1)\n",
    "    model = model.to(device)\n",
    "```\n",
    "\n",
    "Whereas in the DDP code, we would do:\n",
    "\n",
    "```python\n",
    "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "    model = torch.nn.Linear(20, 1)\n",
    "    model = model.to(device)\n",
    "    model = DDP(self.model, device_ids=[self.gpu_id])\n",
    "```\n",
    "\n",
    "That's the only real change we need to make to the model code to enable distributed data parallelism.\n",
    "\n",
    "However, we still have more work to do!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8826fd4b",
   "metadata": {},
   "source": [
    "## Data Loading for Data Parallelism\n",
    "\n",
    "The whole point of data parallelism is to split the data across the different processes, so that each process can work on a different subset of the data in parallel. If we just use the same DataLoader we were using before, every replica will read the whole data set and we'll just end up by doing the exact same training run as before but $P$ times, in parallel.  Plus there'll be communication overhead for the gradient reduction!\n",
    "\n",
    "To avoid this, we need to use a `DistributedSampler` to ensure that each process gets a different subset of the data. The `DistributedSampler` will automatically partition the dataset across the different processes and ensure that each process only sees its own subset of the data.\n",
    "\n",
    "The `DistributedSampler` will also shuffle the data, but it does so in a way that ensures that each process gets a different subset of the data, so we don't need to worry about shuffling the data ourselves.  So if we had `shuffle=True` in our DataLoader before, we can just remove that instead rely on the `DistributedSampler`.\n",
    "\n",
    "So what that looks like for the simple example we'll see next, is that we would change the DataLoader code from:\n",
    "\n",
    "```python\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, shuffle=True)\n",
    "```\n",
    "\n",
    "to: \n",
    "```python\n",
    "    from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=global_rank, shuffle=True)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, sampler=sampler)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c058d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "So basically our steps for taking advantage of DDP are:\n",
    "\n",
    "\n",
    "1. **Initialize the process group**: This sets up the communication backend and initializes the distributed environment.   Get the `RANK`, `LOCAL_RANK`, and `WORLD_SIZE` from the environment variables, which are set by `torchrun` when launching the script, and call `dist.init_process_group()` with the appropriate parameters.\n",
    "2. **Grab the right device**: Use the `LOCAL_RANK` to set the device for each process, so that each process uses a different GPU.\n",
    "3. **Wrap the model**: Use `torch.nn.parallel.DistributedDataParallel` to wrap your model, which will handle the gradient synchronization.\n",
    "4. **Data loading**: Use `torch.utils.data.distributed.DistributedSampler` to ensure that each process gets a unique subset of the data.\n",
    "5. **Training loop**: Very little needs to be done here\n",
    "6. **Synchronize only where necessary, and Gate I/O**: Where absolutely necessary use `dist.barrier()` - but be careful, it's a very expensive operation - and make sure you don't have multiple processes writing to the screen or to a file unless necessary, so you'll probably have `if global_rank == 0` or `if local_rank == 0` checks.\n",
    "5. **Clean up**: Call `dist.destroy_process_group()` at the end of your script to clean up the process group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc594d",
   "metadata": {},
   "source": [
    "## Simple example - from [PyTorch documentation](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)\n",
    "\n",
    "Given what we've discused, let's take a look at the simple example from the Pytorch documentation.  The files are:\n",
    "\n",
    "* [ddp-example-singlegpu.py](code/ddp-example-singlegpu.py)\n",
    "* [ddp-example-multigpu.py](code/ddp-example-multigpu.py)\n",
    "\n",
    "We "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b97576",
   "metadata": {},
   "source": [
    "## EuroSAT example\n",
    "\n",
    "Now we're going to take a look at a still simple but slightly more realistic example - we're going to take the single-GPU EuroSAT classifier traininer we looked at [in a previous notebook](1_EuroSAT_single_gpu.ipynb), and train it with DDP across a varying number of "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b654d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "You can run the [single GPU implementation](code/eurosat_singlegpu.py) \n",
    "below, to see that it performs as with the interactive version \n",
    "we played with before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbef177",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!python3 ./code/eurosat_singlegpu.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57937c3e",
   "metadata": {},
   "source": [
    "We also have a marked-up version with some comments walking you through the process of adding DDP to the training of this model; you can look at it [here](code/eurosat_pipeline_step0.py).  You can run it to see that it gives the same results (all that's changed are some comments):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37630a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./code/eurosat_pipeline_step0.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315a1fe",
   "metadata": {},
   "source": [
    "Lets save a copy of this before we start...  you can always copy it back if something goes wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167fa054",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./code/eurosat_pipeline_step0.py ./code/eurosat_pipeline_orig.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b164ceb",
   "metadata": {},
   "source": [
    "### Step 1 - Adding communications infrastructure.\n",
    "\n",
    "Edit [the code](code/eurosat_pipeline_step0.py) to take care of all of the 'TODO Step 1' steps.  Basically you're setting up the communications infastructure, assigning the appropriate GPUs, and only having one rank print out status updates or write checkpoints:  that means\n",
    "\n",
    "\n",
    "1. **Initialize the process group**: This sets up the communication backend and initializes the distributed environment.   Get the `RANK`, `LOCAL_RANK`, and `WORLD_SIZE` from the environment variables, which are set by `torchrun` when launching the script, and call `dist.init_process_group()` with the appropriate parameters.\n",
    "2. **Grab the right device**: Use the `LOCAL_RANK` to set the device for each process, so that each process uses a different GPU.\n",
    "3. **Synchronize only where necessary, and Gate I/O**: Where absolutely necessary use `dist.barrier()` - but be careful, it's a very expensive operation - and make sure you don't have multiple processes writing to the screen or to a file unless necessary, so you'll probably have `if global_rank == 0` or `if local_rank == 0` checks.\n",
    "4. **Clean up**: Call `dist.destroy_process_group()` at the end of your script to clean up the process group."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
